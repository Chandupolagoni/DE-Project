# Azure Databricks Data Engineering Project

A comprehensive, production-ready data engineering solution built on Azure and Databricks for modern data processing and analytics.

## ğŸ—ï¸ Architecture Overview

This project implements a modern data engineering pipeline with the following components:

- **Azure Data Lake Storage Gen2**: Raw and processed data storage
- **Azure Databricks**: Data processing and transformation engine
- **Azure Data Factory**: Pipeline orchestration and scheduling
- **Azure Key Vault**: Secrets and configuration management
- **Azure SQL Database**: Data warehouse for analytics
- **Azure Monitor**: Logging and monitoring

## ğŸ“ Project Structure

```
â”œâ”€â”€ infrastructure/           # Infrastructure as Code (Terraform)
â”œâ”€â”€ databricks/              # Databricks notebooks and jobs
â”œâ”€â”€ data-factory/            # Azure Data Factory pipelines
â”œâ”€â”€ src/                     # Python source code
â”œâ”€â”€ tests/                   # Unit and integration tests
â”œâ”€â”€ config/                  # Configuration files
â”œâ”€â”€ docs/                    # Documentation
â””â”€â”€ scripts/                 # Deployment and utility scripts
```

## ğŸš€ Quick Start

### Prerequisites
- Azure CLI installed and configured
- Terraform >= 1.0
- Python 3.8+
- Databricks CLI

### 1. Deploy Infrastructure
```bash
cd infrastructure
terraform init
terraform plan
terraform apply
```

### 2. Deploy Databricks Assets
```bash
cd databricks
databricks workspace import_dir . /Shared/data-engineering-project
```

### 3. Run Data Pipeline
```bash
python scripts/run_pipeline.py
```

## ğŸ“Š Data Flow

1. **Ingestion**: Raw data from various sources (CSV, JSON, APIs)
2. **Bronze Layer**: Raw data storage with minimal processing
3. **Silver Layer**: Cleaned and validated data
4. **Gold Layer**: Aggregated and business-ready datasets
5. **Analytics**: Data served to downstream applications

## ğŸ› ï¸ Technologies Used

- **Azure Services**: Data Lake Storage, Databricks, Data Factory, Key Vault
- **Languages**: Python, SQL, Scala
- **Frameworks**: PySpark, Delta Lake, Great Expectations
- **Infrastructure**: Terraform, ARM templates
- **Monitoring**: Azure Monitor, Application Insights

## ğŸ“ˆ Features

- âœ… Automated data ingestion from multiple sources
- âœ… Data quality validation and monitoring
- âœ… Incremental data processing
- âœ… Error handling and retry mechanisms
- âœ… Cost optimization with auto-scaling clusters
- âœ… Security with Azure AD integration
- âœ… Comprehensive logging and monitoring

## ğŸ”§ Configuration

Update the configuration files in the `config/` directory with your Azure subscription details and environment-specific settings.

## ğŸ“š Documentation

Detailed documentation is available in the `docs/` directory:
- [Architecture Guide](docs/architecture.md)
- [Deployment Guide](docs/deployment.md)
- [Git Setup Guide](GIT_SETUP_GUIDE.md)

## ğŸš€ Push to Repository

To push this project to a Git repository:

### Option 1: Use the automated script
```bash
# Windows Batch
push_to_repo.bat

# PowerShell
.\push_to_repo.ps1
```

### Option 2: Manual Git commands
```bash
git init
git add .
git commit -m "Initial commit: Azure Databricks Data Engineering Project"
git branch -M main
git remote add origin YOUR_REPOSITORY_URL
git push -u origin main
```

See [GIT_SETUP_GUIDE.md](GIT_SETUP_GUIDE.md) for detailed instructions.

## ğŸ§ª Testing

```bash
# Run unit tests
make test

# Run integration tests
make test-integration

# Run all tests with coverage
make test-all
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ¯ Project Summary

This project provides a complete, enterprise-grade data engineering solution with:

- **2000+ lines of production code**
- **Complete Azure infrastructure setup**
- **Scalable data processing pipelines**
- **Data quality monitoring**
- **Automated deployment**
- **Comprehensive testing**
- **Full documentation**

See [PROJECT_SUMMARY.md](PROJECT_SUMMARY.md) for a complete overview of what's included.

## ğŸ“ Support

For questions or issues:
- Check the documentation in the `docs/` directory
- Review the troubleshooting section in `docs/deployment.md`
- Examine the test cases for usage examples